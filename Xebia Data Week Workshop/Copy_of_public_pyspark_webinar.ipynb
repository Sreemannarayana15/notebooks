{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QONCIQ49Imuv"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=17aU5CN_8dGvyZI89lNTvtydVJ6PNZBdU)\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUijaxWpImu0"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1pWDTauVuTj4nRcjQNSF2fZ-rocpJtecU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD6L4_KFImu0"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1YLdS-5ae_WjK4ux7K4dRRKG3CTIkIt40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwZoE0l2Imu0"
      },
      "source": [
        "# üëã Help me get to know you"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX1QVMRFImu0"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1kcK7ocsz1ZK6HgtZsAT5ICIND7xn6JUM)\n",
        "\n",
        "https://xebia.ai/scalable-ml\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I84uaPBOImu1"
      },
      "source": [
        "# ‚úÖ Agenda\n",
        "\n",
        "**This webinar is a preview of a course that is under development**\n",
        "\n",
        "<br> This course is for you, if:\n",
        "\n",
        "‚úî You have no prior experience with distributed ML, but want to change that?\n",
        "\n",
        "‚úî You are interested in scaling your machine learning workflows across a cluster.\n",
        "\n",
        "‚úî You want to understand the practical challenges involved in scaling machine learning workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíª Code-along\n",
        "\n",
        "Our trainings are designed to be hands-on and practical. <br> Please click on the link in the chat to access the notebook used for the webinar. <br> If you have questions, please add them to the Q&A, and we will discuss them at the end of the session.\n",
        "\n",
        "https://colab.research.google.com/drive/1M88L3pb-uGG89pfRDjwA91PskHGl7cec?usp=sharing\n"
      ],
      "metadata": {
        "id": "m8JnFAn10QaZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiB6SulfImu2"
      },
      "source": [
        "# Scaling ML Workflows\n",
        "\n",
        "As a data scientist, scaling your ML workflows is a crucial aspect of the machine learning model development lifecycle, especially when your data gets too big (memory bound), or your models get too complex (compute bound). üß∞\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1BR7lg1KWnyessf29bGGI2ZvRCR1TUON6)"
      ],
      "metadata": {
        "id": "P1s9zTD_Qjvq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N2L399gImu2"
      },
      "source": [
        "## Scaling paradigms\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=18fijm3KbNG8ShWX0StHEeH6BJHRNZTP1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GBGRVYlImu2"
      },
      "source": [
        "## Intermezzo: Do you need to scale your ML training? tldr: it depends üì¢\n",
        "\n",
        "&#x21AA; It‚Äôs worth emphasizing that not everyone needs distributed model training. Tools like [sampling](https://en.wikipedia.org/wiki/Sampling_(statistics)) can sometimes be effective. Moreover, always plot your [learning curve](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html). However, the same can't be said for the other steps in your ML pipeline.\n",
        "\n",
        "&#x21AA; e.g., hyperparameter search : tools like [hyperopt](http://hyperopt.github.io/hyperopt/scaleout/spark/) can distribute hyperparameter tuning and parallelize it's search across a cluster.\n",
        "\n",
        "\n",
        "&#x21AA; e.g., inference : Pandas UDFs allow you to use your favourite libraries (sklearn and pandas) while getting the benefits of parallelization and distribution on a spark cluster.\n",
        "\n",
        "\n",
        "&#x21AA; That being said, natively distributed tools like SparkML are worth exploring if you are already working in the spark ecosystem (e.g., via a vendor like Databricks).\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHvmQ2PHImu2"
      },
      "source": [
        "## Apache Spark Ecosystem\n",
        "\n",
        "Apache Spark is an open-source analytics engine for big data processing and machine learning. <br> The top level ML API abstracts away a lot of the Spark internals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=112rxhy7DkmecFWGCClx3J-UfaUULizlZ)\n",
        "***"
      ],
      "metadata": {
        "id": "CR8otMWDRviW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EkSLzT_Imu2"
      },
      "source": [
        "## Spark ML Demo: Predicting house prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGVKpRfwImu3"
      },
      "source": [
        "Let's now install `pyspark`, a high level python API for Spark."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the line below to install pyspark\n",
        "\n",
        "#!pip install pyspark"
      ],
      "metadata": {
        "id": "xo9-r6dw3YEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first need to set up a **`Spark Session`**. <br> A SparkSession is the entry point to interact with the spark internals."
      ],
      "metadata": {
        "id": "YPbesnmXWz9i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbGA8sUMImu3"
      },
      "outputs": [],
      "source": [
        "# set up a spark session using pyspark\n",
        "# the master is set to local[*] to use all available cores\n",
        "\n",
        "import pyspark\n",
        "\n",
        "master = 'local[*]'\n",
        "\n",
        "spark = (\n",
        "    pyspark.sql.SparkSession.builder\n",
        "    .master(master)\n",
        "    .getOrCreate()\n",
        ")\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYPwcrvIImu3"
      },
      "source": [
        "### Let's now read in the dataset üè†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNldS4uXImu4"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "california_df = fetch_california_housing(as_frame=True)\n",
        "california_sdf = spark.createDataFrame(california_df['frame'])\n",
        "california_sdf.show(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqOAzvwBImu4"
      },
      "source": [
        "We can look at the description of the (sklearn) dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAzC0-9NImu4"
      },
      "outputs": [],
      "source": [
        "print(california_df.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp3loD-WImu4"
      },
      "source": [
        "Let's visualize the spatial patterns present in the dataset, who would say no to a house by the beach? üèñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDPjj43gImu4"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.scatterplot(\n",
        "    data=california_df['frame'],\n",
        "    x=\"Longitude\",\n",
        "    y=\"Latitude\",\n",
        "    size=\"MedHouseVal\",\n",
        "    hue=\"MedHouseVal\",\n",
        "    palette=\"viridis\",\n",
        "    alpha=0.5,\n",
        ")\n",
        "plt.legend(title=\"MedHouseVal\")\n",
        "plt.title(\"Median house value based on\\n their spatial locations\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXikydPPImu4"
      },
      "source": [
        "### Fitting a model üõ†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNGove8IImu4"
      },
      "source": [
        "&#x1F4A1; Spark.ml (by default) expects the **label** or **target** variable to be in a column named `label`. We can use the `withColumnRenamed` method that comes with the DataFrame API to rename our columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiHn466fImu5"
      },
      "outputs": [],
      "source": [
        "california_sdf = california_sdf.withColumnRenamed(existing=\"MedHouseVal\", new=\"label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now split the data into a training and a test dataset. We can use the `randomSplit` method that comes with the DataFrame API to rename our columns."
      ],
      "metadata": {
        "id": "TlP2ZhdCc1NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sdf, test_sdf = california_sdf.randomSplit([0.8, 0.2], seed=707)"
      ],
      "metadata": {
        "id": "co6z0SgJc4zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsPpKT2zImu5"
      },
      "source": [
        "&#x1F4A1; Spark.ml expects all features in a **vector**. This can be done using a `VectorAssembler`.\n",
        "\n",
        "By default a model assumes that the vector is in a column called `features`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncuUY-DHImu5"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "feature_cols = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\"]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "train_sdf_vector = (\n",
        "    assembler.transform(train_sdf)\n",
        "    .select(['features', 'label'])\n",
        ")\n",
        "\n",
        "test_sdf_vector = (\n",
        "    assembler.transform(test_sdf)\n",
        "    .select(['features', 'label'])\n",
        ")\n",
        "\n",
        "train_sdf_vector.show(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia-5dCglImu5"
      },
      "source": [
        "Training our model is now a familiar ```.fit()``` away"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_5g-DoLImu5"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# build the model object\n",
        "regressor = LinearRegression()\n",
        "\n",
        "# fit the model object\n",
        "model = regressor.fit(train_sdf_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAylaPKTImu5"
      },
      "source": [
        "What would the model predict for the houses in the test set?\n",
        "\n",
        "We get predictions with `.transform()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMPdJezFImu5"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test_sdf_vector)\n",
        "predictions.show(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL-6sUhfImu5"
      },
      "source": [
        "We can evaluate the model using the **`Evaluator`** class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8AnTjc9Imu6"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# build the evaluator object\n",
        "evaluator = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')\n",
        "\n",
        "# evaluate the predictions\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f'RMSE: {rmse:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBrCmZmaImu6"
      },
      "source": [
        "## Recap\n",
        "\n",
        "What did we just do?\n",
        "\n",
        "&#x2705; Converted a pandas df to a spark df <br>\n",
        "&#x2705; __Preprocessed__ our data <br>\n",
        "&#x2705; __Split__ our data into training and test sets with `randomSplit()` <br>\n",
        "&#x2705; __Assembled__ our data with the `VectorAssembler` <br>\n",
        "&#x2705; __Initialized__ a model and __trained__ it with `.fit()` <br>\n",
        "&#x2705; __Predicted__ with `.transform()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nPpGs2qImu6"
      },
      "source": [
        "## Exercise: Use other regressors and metrics\n",
        "\n",
        "&#x1f4d6; Replace the Linear Regressor with other flavours and evaluate the performance. Click [here](https://spark.apache.org/docs/latest/ml-classification-regression.html#regression) to explore the other models.<br>\n",
        "&#x1f4d6; In addition to the `rmse`, use the `r2` [metric](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.evaluation.RegressionMetrics.html) to evaluate your model. <br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#your solution here"
      ],
      "metadata": {
        "id": "ZuWrYTCxg4K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfXGSZGtImu7"
      },
      "source": [
        "# Pipelines API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI0Go59pImu7"
      },
      "source": [
        "Spark ML exposes a **`Pipeline`** API, which can be used to encapsulate your ML workflows. <br>\n",
        "\n",
        "\n",
        "&#x1F4A1; For people who have used sklearn: a lot of the concepts are similar to (derived from) those from sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIGW5mjgImu8"
      },
      "source": [
        "## Pipeline API Abstractions\n",
        "\n",
        "Build ML workflows as a chain of Transformers and Estimators.\n",
        "\n",
        "Major classes:\n",
        "- __Transformer__: transforms one DataFrame into another DataFrame.\n",
        "- __Estimator__: fits on a DataFrame and produces a Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwNcOUYJImu9"
      },
      "source": [
        "## Transformers\n",
        "\n",
        "Transform one DataFrame into another.\n",
        "\n",
        "__Examples__:\n",
        "\n",
        "- Feature engineering\n",
        "    - Compute new/derived features, adding extra columns.\n",
        "    - Assemble features into feature vectors.\n",
        "- Learned models - predict response for given dataset, adding a new column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjkvO9gyImu9"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Estimator, Transformer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "feature_cols = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\"]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "\n",
        "california_sdf_vector = (\n",
        "    assembler.transform(california_sdf)\n",
        "    .select(['features', 'label'])\n",
        ")\n",
        "\n",
        "print('assembler is Estimator:', isinstance(assembler, Estimator))\n",
        "print('assembler is Transformer:', isinstance(assembler, Transformer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKiiaJmjImu9"
      },
      "source": [
        "## Estimators\n",
        "\n",
        "Are fit on a DataFrame and produce a Transformer.\n",
        "\n",
        "__Examples__:\n",
        "\n",
        "- Feature engineering\n",
        "    - Transformations that learn mapping from dataset\n",
        "    - E.g., QuantileScaler, OneHotEncoderEstimator\n",
        "- Models - fit model on given dataset, producing a learned model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyABU6K5Imu9"
      },
      "source": [
        "For example, a `LinearRegression` Estimator doesn't know how to transform as it has to be fit first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz1DrT09Imu9"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "regressor = LinearRegression()\n",
        "\n",
        "print('regressor is Estimator:', isinstance(regressor, Estimator))\n",
        "print('regressor is Transformer:', isinstance(regressor, Transformer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUoeV6itImu9"
      },
      "source": [
        "After fitting it knows how to predict and a `Transformer` is returned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhZ2AikNImu-"
      },
      "outputs": [],
      "source": [
        "fitted_regressor = regressor.fit(california_sdf_vector)\n",
        "\n",
        "print('fitted_regressor is Estimator:', isinstance(fitted_regressor, Estimator))\n",
        "print('fitted_regressor is Transformer:', isinstance(fitted_regressor, Transformer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdsftRmbImu-"
      },
      "source": [
        "Once we have a fitted model, making predictions is nothing more than a transform: a DataFrame gets returned after prediction.\n",
        "\n",
        "For example, calling `transform` on our model returns a `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmgnkuG8Imu-"
      },
      "outputs": [],
      "source": [
        "transformed = fitted_regressor.transform(california_sdf_vector)\n",
        "transformed.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03Sgk7PjImu-"
      },
      "source": [
        "## Pipeline: Glue everything together\n",
        "\n",
        "- A Pipeline is an Estimator that chains other Tranformers and Estimators.\n",
        "- Pipelines return a Transformer after fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mB1MmV09Imu_"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.pipeline import Pipeline\n",
        "\n",
        "# Initialize assembler and regressor.\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "regressor = LinearRegression()\n",
        "\n",
        "# Combine steps in a single pipeline.\n",
        "pipeline = Pipeline(stages=[assembler, regressor])\n",
        "fitted_pipeline = pipeline.fit(train_sdf)\n",
        "\n",
        "print('pipeline is Transformer:', isinstance(pipeline, Transformer))\n",
        "print('fitted_pipeline is Transformer:', isinstance(fitted_pipeline, Transformer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Axwxi0C4Imu_"
      },
      "outputs": [],
      "source": [
        "with_predictions = fitted_pipeline.transform(test_sdf)\n",
        "with_predictions.select([\"features\", \"label\", \"prediction\"]).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEt7PzFqImu_"
      },
      "source": [
        "# Model selection & evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdaI083pImu_"
      },
      "source": [
        "## Model selection\n",
        "\n",
        "Model selection has to do with:\n",
        "\n",
        "- finding the best hyper parameters for a model;\n",
        "- finding the best model from various types;\n",
        "- and making sure you've validated that properly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKSeAG6CImu_"
      },
      "source": [
        "### Cross Validation\n",
        "\n",
        "Estimate the generalization error of your model by simulating multiple experiments:\n",
        "\n",
        "<img src=\"http://i.stack.imgur.com/1fXzJ.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUSXo_C6Imu_"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "SparkML allows us to distribute hyperparameter tuning when using a SparkML Estimator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3e6tuZDImvA"
      },
      "source": [
        "First, we can create a model and a grid to search over:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPRLg1tmImvA"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "\n",
        "regressor = LinearRegression()\n",
        "evaluator = RegressionEvaluator(metricName='rmse')\n",
        "\n",
        "reg_param = [0.001, 0.003, 0.005, 0.008, 0.01]\n",
        "grid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(regressor.regParam, reg_param)\n",
        "    .build()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJlcQVJhImvA"
      },
      "source": [
        "Next, we create a `CrossValidator` and set the model, evaluator and grid.\n",
        "\n",
        "`cv_model` now contains our best model based on the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "mEsXfR2XImvA"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import CrossValidator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "cv = CrossValidator(numFolds=4,\n",
        "                    estimator=regressor,\n",
        "                    estimatorParamMaps=grid,\n",
        "                    evaluator=evaluator)\n",
        "\n",
        "cv_model = cv.fit(train_sdf_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rwQO9zVImvA"
      },
      "source": [
        "How many models are we training in this example? And what was the optimal value for the `regParam` hyperparameter?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwRxz0qSImvA"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(reg_param, cv_model.avgMetrics, '-o')\n",
        "ax.set_xlabel('regParam')\n",
        "ax.set_ylabel('rmse');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cv_model.bestModel.explainParams())"
      ],
      "metadata": {
        "id": "3orAUWlsGdwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0MQnmajImvA"
      },
      "source": [
        "What's the score on the test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5MxP0oXImvA"
      },
      "outputs": [],
      "source": [
        "predictions = cv_model.transform(test_sdf_vector)\n",
        "\n",
        "evaluator = RegressionEvaluator()\n",
        "print('rmse: ', evaluator.evaluate(predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Would you be interested in knowing more about...\n",
        "\n",
        "‚úÖ scaling up feature engineering\n",
        "\n",
        "‚úÖ distributed hyperparamter tuning\n",
        "\n",
        "‚úÖ distributed model inference\n",
        "\n",
        "‚úÖ scaling single node model workflows\n",
        "\n",
        "‚úÖ using mlflow in a distributed workflow\n",
        "\n",
        "‚úÖ and more concepts in our lab sandboxes simulating a real-world prod environment\n",
        "...\n",
        "\n",
        "## ... then please contact our training advisor, Rozaliiya\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=1csdtbnC3MFa7LvGYw2Zzz7k3UIo0Y7dG)\n",
        "\n",
        "\n",
        "https://xebia.ai/rozaliia"
      ],
      "metadata": {
        "id": "BeBaDAjow-B8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your opinion matters to us\n",
        "\n",
        "![](https://drive.google.com/uc?id=1kcK7ocsz1ZK6HgtZsAT5ICIND7xn6JUM)\n",
        "\n",
        "https://xebia.ai/scalable-ml"
      ],
      "metadata": {
        "id": "TyGZ47sp1dZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank you!\n",
        "\n",
        "üí° We love to share knowledge, check out our [blog](https://xebia.com/blog/category/topics/data-science-and-ai/) to find out what we've been upto!"
      ],
      "metadata": {
        "id": "g2epMPkaWgBr"
      }
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}